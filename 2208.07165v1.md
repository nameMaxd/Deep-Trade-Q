Deep Reinforcement Learning Approach for
Trading Automation in The Stock Market
Taylan Kabbani1, Ekrem Duman 2
Department of Industrial Engineering, Ozyegin University, Istanbul, Turkey
1taylan.kabbani1@ozu.edu.tr
2ekrem.duman@ozyegin.edu.tr
Abstract—Deep Reinforcement Learning (DRL) algorithms can investor,wherethetradingagent(thealgorithm)interactswith
scale to previously intractable problems. The automation of profit the environment (the model) to take the optimal decision [6].
generationinthestockmarketispossibleusingDRL,bycombining
In addition, financial data is highly time-dependent (function
the financial assets price ”prediction” step and the ”allocation” step
oftime),makingitaperfectfitforMarkovDecisionProcesses
of the portfolio in one unified process to produce fully autonomous
systemscapableofinteractingwithitsenvironmenttomakeoptimal (MDP)[7],whichisthecoreprocessofsolvingRLproblems.
decisionsthroughtrialanderror.ThisworkrepresentsaDRLmodel MDP captures the entire past data and defines the whole
togenerateprofitabletradesinthestockmarket,effectivelyovercom- history of the problem in just the agent’s current state, and
ing the limitations of supervised learning approaches. We formulate
that’s highly crucial when it comes to modeling financial
thetradingproblemasaPartiallyObservedMarkovDecisionProcess
market data [8]. (POMDP) model, considering the constraints imposed by the stock
market, such as liquidity and transaction costs. We then solve the Most works that studied the RL’s applications in financial
formulated POMDP problem using the Twin Delayed Deep Deter- markets and particularly in trading stocks considered discrete
ministic Policy Gradient (TD3) algorithm reporting a 2.68 Sharpe action spaces [9], [10], [11], [12], i.e., buy, hold, and sell a
Ratioonunseendataset(testdata).Fromthepointofviewofstock
fixed number of shares to trade a single asset. In this work, a
market forecasting and the intelligent decision-making mechanism,
continuousactionspaceapproachisadoptedtogivethetrading
this paper demonstrates the superiority of DRL in financial markets
over other types of machine learning and proves its credibility and agent the ability to gradually adjust the portfolio’s positions
advantages of strategic decision-making. with each time step (dynamically re-allocate investments),
resulting in better agent-environment interaction and faster
Keywords—Autonomous agent, Deep reinforcement learning,
MDP, Sentiment analysis, Stock market, Technical indicators, Twin convergenceofthelearningprocess.Inaddition,theapproach
delayed deep deterministic policy gradient supportsthemanagingofaportfoliowithseveralassetsinstead
of a single one. We first propose a novel formulation of the
stock trading problem or what is referred to as the trading
I. INTRODUCTION
EnvironmentasaPartiallyObservedMarkovDecisionProcess
THE prime objective of any investor when investing in (POMDP) model considering the constraints imposed by the
any financial market is to minimize the risk involved stock market, such as liquidity and transaction costs. More
in the trading process and maximize the profits generated. specifically,wedesignanenvironmentthatsimulatesthereal-
Investorscanmeetthisobjectivebysuccessfullypredictingthe world trading process by augmenting the state (observation)
prices or trends of the market assets and optimally allocating representation with ten different technical indicators and sen-
the capital among the selected assets. This process is very timent analysis scores of news releases along with other state
challenging for a human to consider all relevant factors in components. We then solve the formulated POMDP problem
a complex and dynamic environment; therefore, the design using the Twin Delayed Deep Deterministic Policy Gradient
of adaptive automated trading systems capable of meeting (TD3)algorithm,whichcanlearnpoliciesinhigh-dimensional
the investor’s objective and bringing more stagnant wealth and continuous action spaces like those typically found in the
into the global market has been an intensive research topic. stock market environment. Finally, we evaluate our proposed
Many efforts have been made to design such trading systems approach by performing back-testing, which is the process
in the past decade. The majority of these efforts focused on usedbytradersandanalyststoasserttheviabilityofatrading
using Supervised learning (SL) techniques [1], [2], [3], [4], strategy by testing it on historical data.
[9], which in essence train a predictive model (e.g., Neural
Network, Random Forest,...) on historical data to forecast the
II. BACKGROUNDANDRELATEDWORK
trend direction of the market. Regardless of their popularity,
A. MDP in Reinforcement Learning
these techniques suffered from various limitations, leading to
sub-optimal results [5]. Reinforcement Learning (RL) offers In essence, Markov Decision Processes [13] (MDP) is used
to solve the drawbacks of Supervised Learning approaches to model stochastic processes containing random variables,
in trading financial markets by combining the financial assets transitioning from one state to another depending on certain
price ”prediction” step and the ”allocation” step of the port- assumptions and definite probabilistic rules. MDPs are a
folio in one unified process to optimize the objective of the perfectmathematicalframeworktodescribethereinforcement
2202
luJ
5
]RT.nif-q[
1v56170.8022:viXra
learning problem. In this framework, researchers call the From Bellman equations (Eq. 3 and Eq. 4) we can derive
learnerordecisionmakertheagentandthesurroundingwhich what is called The Bellman Optimality Equations. Intuitively,
the agent interacts with (comprising everything outside the the Bellman optimality equation expresses the fact that the
agent) the environment. The learning process ensues from value of a state under an optimal policy (π ) must equal the
∗
the agent-environment interaction in MDP, at each time step expected return for the best action from that state [14], and
t ∈ {1,2,3,...,T} the agent receives some representation the optimal state-value function (V ) equals to :
∗
(information)ofitscurrentstatefromtheenvironments t ∈S, V (s)=max(cid:88)(cid:88) P(s(cid:48),r|s,a)[r+γV (s(cid:48))] (5)
and on that basis selects an action a ∈ A to perform. One ∗ ∗
t a
steplater,duetoitsaction,theagentfindsitselfinanewstate, s(cid:48) r
and the environment returns a reward R
t+1
∈R to the agent Similarly, we define optimal action-value (q ∗) function as:
as a feedback of its action’s quality [14]. (cid:88)(cid:88)
q (s,a)=maxq (s,a)= P(s(cid:48),r|s,a)[r+γmaxq (s(cid:48),a(cid:48))]
∗ π ∗
π a(cid:48)
s(cid:48) r
(6)
B. The Objective of Reinforcement Learning
The objective of any RL problem is to maximize the
cumulative reward G it receives in the long run instead of D. Taxonomy of RL Algorithms
t
the immediate reward R RL algorithms are classified based on how to represent and
t
train the agent into three main approaches:
E[G t]=E[R t+1+R t+2+R t+3+...+R T] (1) 1) Critic-Only Approach: This family of algorithms learns
to estimate the value function (State-value function or action-
In the above reward equation (Eq. 1), the term R denotes
T
value function) by using what is called Generalized Policy
the reward received at the terminal state T. meaning that the
Iteration (GPI), this concept refers to the interaction of two
aforementioned equation is only valid when the problem at
steps. The first step is the policy-evaluation, the main goal
hand is an Episodic task, i.e., ends in a terminal state T. For
of this step is to collect information (value functions) under
theContinuoustasksi.e.,noterminalstate,T =∞,adiscount
the given policy to determine how good it is. The second
factor gamma is introduced to Eq. 1 (0≤γ ≤1):
step is the policy-improvement, it is responsible of improving
G =R +γR +γ2R +...+γk−1R +.... the policy by choosing greedy actions with respect to the
t t+1 t+2 t+3 t+k
valuefunctionscomputedfromthepolicy-evaluationstep.The
∞
=(cid:88) γkR (2) two steps alternate in a consecutive manner until the value
t+k+1
functions and policies stabilize which means that the process
0
has reached an optimal policy as illustrated in Fig. 1.
C. Bellman Equations
Value functions are being used by almost all RL methods
to estimate how good (in terms of expected return) it is for
the agent to be in a given state or to perform an action in
a given state. This evaluation is being made based on the
future expected sum of rewards. Accordingly, value functions
are determined with respect to the future actions the agent
will take. We call a particular way of acting a Policy (π) [14]
which is a function that maps from environment’s states to
probabilities of selecting each possible action.
Bellman equations [15] are the fundamental property of
value functions used in dynamic programming as well as in
reinforcement learning to solve MDPs, and they are essen- Fig.1. GeneralizedPolicyIteration[14]
tial to understand how many RL algorithms work. Bellman
We distinguish between two different ways the agent learns
equation states that the value function of state s (V (s))
π
the value function of the system. The first way is Tabular
can be calculated by finding the sum over all possibilities
Solution Method where the value functions are represented as
of expected returns, weighting each by its probability of
arrays or tables and updated with more accurate values after
occurring following a policy π:
eachiterationastheagentcollectsmoreexperience.Thisway
. (cid:88) (cid:88)(cid:88)
V (s)= π(a|s) P(s(cid:48),r|s,a)[r+γV (s(cid:48))],∀s∈S of learning often finds exact solutions. However, it does not
π π
a s(cid:48) r generalize well, and the state and action spaces must be small
(3)
enough to be stored in tables.
Inasimilarwaywedefinetheaction-value(q (s,a))function
π The second possible way in the critic-only approach is
as:
calledApproximateSolutionMethod,whichtendstogeneralize
q (s,a)=(cid:88)(cid:88) P(s(cid:48),r|s,a)[r+γ(cid:88) π(a(cid:48)|s(cid:48))q (s(cid:48),a(cid:48))] betterthanTabularMethodbuthaslowerdiscrimination,andit
π π
iscapableoflearningthevaluefunctionofsystemswithenor-
s(cid:48) r a(cid:48)
(4) mous state and action spaces. Approximate methods achieve
thisgeneralizationbycombiningRLwithsupervisedlearning selected Italian stock. Specifically, they compared the perfor-
algorithms. Deep Reinforcement Learning is considered an mance of Q-learning, and Kernal-based reinforcement learn-
approximatemethodthatcombinesNeuralNetworkswithRL. ing, concluding that Q-learning performance outperformed
Mnihetal.(2013)[16]isconsideredthefatherofDRL,where Kernal-based RL. In a subsequent study (2014) [10], they
he trained an agent of Deep Q-network (DQN) to play Atari exploredtheeffectofdifferentrewardfunctionssuchasSharpe
games, where pixels of the game screen were the input data ratio, average log return, and OVER ratio on the performance
(state), and the directions of the joystick were actions. He of Q-learning. By trading six selected Italian stocks, they
proved that DRL had outperformed all existing algorithms in reportedthatlaggedreturnrewardfunctionhasthebestperfor-
2015 [17]. mance.Insteadofapproximatingavaluefunction(critic-only),
2) Actor-Only Approach: All methods under the Critic- Dengetal.(2017)[12]madeoneofthefirstattemptsoncom-
Only approach rely on the GPI framework to learn approxi- biningDeepLearningwithRecurrentReinforcementLearning
mateactionvaluestoinferagoodpolicy.Actor-Onlymethods to directly approximate a policy function. This approach is
(alsocalledPolicyGradientMethods)estimatethegradientof called “deep recurrent reinforcement learning” (DRRL). In
the objective which is maximizing rewards with respect to the their proposed method, first, the DL part extracts 45 useful
policyparametersandadjustthepolicyparametersθ basedon features from the market to be used as state representative
the estimate (Eq. 7). The parameterized policy function takes in the environment. Secondly, they use a Recurrent Neural
state and action as an input and returns the probability of Network (RNN) as a trading agent to interact with the deep-
taking that action in that state instead of taking the state only generated state features and make decisions. To investigate
as an input and returning the value function as Critic-Only the potential advantage of Actor-Critic methods in solving
methods do. Note that in the bellow equation G represents the day trading problem, Conegundes and Pereira (2020) [22]
t
the expected reward at time t. used Deep Deterministic Policy Gradient (DDPG) algorithm
to solve the asset allocation problem. Considering different
θ =θ +α∇lnπ(a |s ,θ )G (7) constraints such as liquidity, latency, slippage, and transaction
t+1 t t t t t
costs, they back-tested their approach on the Brazilian Stock
3) Actor-CriticApproach: IntheActor-Criticapproach,the Exchange datasets. They showed that their approach success-
actor’s job is to select actions at each time step to form the fully obtained 311% cumulative return in three years with an
policy,wherethecritic’sroleistoevaluatetheseactionstaken annual average maximum drawdown around 19%.
by the actor. So the approach is gradually adjusting the policy
parametersθoftheactortotakeactionsthatmaximizethetotal III. PROBLEMDESCRIPTION
reward predicted by the critic. The TD error (δ) calculated by
The stock trading problem is being modeled as Partially
the critic to evaluate the action is as follows:
Observed Markov Decision Process (POMDP), which can be
δ =R +γVˆ(s ,w)−Vˆ(s ,w) (8) formulated by describing its State Space, Action Space, and
t+1 t+1 t
RewardFunction.ThePOMDPmodeloftheproblemiscalled
The value function estimation of the current state Vˆ(s ,w) is the trading environment, and it’s built to carefully mimic the
t
addedasabaselinetomakethelearningfaster.Theparameter real-world trading process.
θ of the actor is being adjusted in the way of maximizing the
total future reward from Eq. 7 and Eq. 8 we conclude the A. State Space
equation used by the Actor-Critic to update the gradient at
The state-space in the proposed environment is designed
each time step t as the following:
to support multiple and single stock trading by representing
θ =θ +α∇lnπ(a |s ,θ )(R +γVˆ(s ,w)−Vˆ(s ,w)) the state as (1+ 13 x N)-dimensional vector where N is the
t+1 t t t t t+1 t+1 t
number of assets we consider to trade in the market. Hence
ManyresearchersworkedonimprovingtheDQNalgorithm. the state space increases linearly with the number of assets
Van Hasselt et al. (2015) [18] proposed to use two networks available to be traded.
instead of one Q-network to choose the action and the other There are two main parts of the state presentation. The first
to evaluate the action taken to solve the deviation problem in partisthePositionState∈R1+N whichholdsthecurrentcash
+
DQN.TheycalleditDouble-DQN.Lillicrapetal.(2018)[19] balance and shares owned of each asset in the portfolio, and
built on the top of Double-DQN, an algorithm based on the the second part of the state is the Market Signals ∈ R12×N,
deterministic policy gradient (DDPG) that can operate over which holds the necessary market features for each asset as a
continuous action spaces. The Twin Delayed Deep Determin- tuple, these features are the required information provided to
isticPolicyGradient(TD3)algorithmwhichwillbediscussed the agent to make predictions of the market movement. The
in section IV-A, was proposed by Fujimoto et al. (2019) [20] firsttypeofinformationisbasedonthehypothesisoftechnical
to tackle the problem of the approximation error in DDPG. analysis[23],whichstatesthatthefuturebehavioroffinancial
markets is conditioned on its past; hence technical indicators
are being used in the state space to help the agent interpret
E. RL in Finance
the market behavior. The second type of information is based
Bertoluzzo and Corazza (2012) [21] investigated the per- on fundamental analysis [24], which studies everything from
formance of different RL algorithms in day-trading for one the overall economy and industry conditions to news releases.
Therefore a Natural Language Processing (NLP) approach Where:
is used to measure the general sentiment from the news * N: assets in the portfolio.
releasesandintegrateitwiththestaterepresentation.Thestate * A :theactionvectorsentbytheagenttotheenvironment.
t
(observation) vector at each time step is provided to the agent * ai: the action (number of shares) to buy/sell for asset i
t
as follows: at time step t.
* K : the maximum number of shares the agent can re-
max
S t =[[b t,h t],[{(Ci t,SSi t,Ti t)|i∈N}]] allocate of an individual asset at each time step t.
* hi: the portfolio position (number of shares) of asset i at
Each component of the state space is defined as follows: t
time step t.
• N ∈ZN +: Number of assets in the portfolio.
The action space depends on the number of assets available
• b t ∈ R +: The available cash balance in the portfolio at in the portfolio N and it’s given as (2×K +1)N; hence
time step t. max
the action space increases exponentially by increasing N.
• h t ={hi t|i∈N}={h0 t,h1 t,...,hN t }∈ZN +:Thenumber
of shares owned for each asset i in N at time step t.
• Ci t ∈RN +: The close price of asset i in N at time step t. C. Reward Function
• SSi t ∈ (−1,0,1): An integer 1, 0 or -1 to indicate the The difference between the portfolio value V t at the end
sentiment of the news related to stock i at time step t. of period t and the value at the end of previous period t−1
• Ti t:The10differentTechnicalIndicatorsvectorforasset represents the immediate reward r(s,a,s(cid:48)) received by the
i in the portfolio at time step t using the past prices of agent after each action, and we denote the final investment
theassetinaspecifiedlook-backwindow(mostcommon return at a target time T as G.
f
window is 14 or 9).
r(s,a,s(cid:48))=V −V (10)
To demonstrate the state space, let’s assume that we have 3 t t−1
different assets (N = 3) in the trading environment and an Where the portfolio value V at each time step is calculated
initial capital of 1000$ to be invested, the state vector would as:
be a 40-dimensional vector and the initial state(s ) given by V =b +h .C (11)
0 t t t t
the environment would be:
Where:
s 0 =[[1000,0,0,0][(p1 0,SS 01,T 01),(p2 0,SS 02,T 02),(p3 0,SS 03,T 03)]] * b t:theavailablecashbalanceintheportfolioattimestep
t.
B. Action Space * h t ={hi t|i∈N}: the position vector (number of shares
of each asset) at time step step t.
The designed agent in this study receives the state s
t * C ={Ci|i∈N}: the closing price of each asset in the
at each time step t as input and sends back action in the t t
portfolio at time step t.
range between 1 and -1 inclusive, a ∈ [−1,1], the action
t
The transition cost can be represented in many different
then is re-scaled using a constrain K , which represents
max
ways in real life, and it varies from one broker to another.
the maximum allocation (buy/sell shares), transforming a
t
To better simulate the real-world trading process in the stock
to an integer K ∈ [−K ,....,−1,0,1,....,K ], which
max max
market, transaction costs (i.e., commission fees) are incorpo-
stands for the number of shares to be executed, resulting in
rated into the immediate reward (r(s,a,s(cid:48))) calculation. In
decreasing,increasingorholdingofthecurrentpositionofthe
this study, we set the commission as a fixed percentage of
corresponding asset [25]. There are two important conditions
the total closed deal cash amount, where d represents the
regarding the action execution in our approach: buy
commission percentage when buying is performed, and d
• If the current capital (cash) in the portfolio is insufficient sell
is the commission percentage for selling:
to execute the buy action, the action will be partially
executed with what the current capital can buy of the d ={di|i∈N}=[d0,d1,...,dN]
t t t t t
requested stock. 
d , if ai >0
• I pf ort th fe olin oum isbe ler so sf thsh anare ts hefo nr ua mbsp ee rc oifi fc shas as re et s( th oi t) bein soth lde
where:di
t
= 0,buy
if
at
i
t
=0
(ai ∈Z−), the agent will sell all the remaining shares of d , if ai <0
t sell t
this asset in the portfolio.
The commission vector d is incorporated into the immediate
t
Wecanmathematicallyexpresstheactionspaceasthefollow-
reward function by excluding the commission amount paid
ing:
from the portfolio value calculated in Eq. 11, so the agent
A ={ai|i∈N}={a0,a1,...,aN} (9)
t t t t t would avoid excessive trading that results in a high commis-
sion rate and therefore avoids a negative reward:
S.t.
ai ∈ZN V t =b t+h t.C t−h t.(C t−1◦d t) (12)
t
−K ≤ai ≤K , ∀i∈N In the above equation, the amount paid for the commission is
max t max calculatedbytakingtheHadamardproductofthecommission
ai =hi if |ai|>hi, ∀a ∈Z− vector d and the closing price of the previous period C ,
t t t t t t t−1
that’s because the action of buying/selling occurred on the overestimation bias, thus reducing the accumulation of errors
previous state and therefore commission should be calculated in the learning process by introducing three main components
using the closing prices on that state. to DDPG:
1) Clipped Double Critic Networks: The first component
D. Environment Constraints and Assumptions added is a novel clipped variant of Double Q-learning
[18] to replace the single critic. Using two different and
We impose the following constraints and assumptions on
separatecriticnetworkstomakeanindependentestimate
the MDP environment for two main reasons. First, to idealize
of the value function can be used to make unbiased
and simplify the complex financial market systems (e.g., via
estimatesoftheactionsselectedusingtheoppositevalue
liquidityassumption)withoutlosingthenatureoftheproblem,
estimate. TD3 uses a clipped double Q-learning instead
and the second reason is to make the model closer to a real-
of the traditional one used in Double Q-learning where
world situation.
it takes the smallest value of the two critic networks
1) Non-Negative Balance Constraint: The cash balance in
estimates, that is, if we use the traditional Double Q-
any state is not allowed to be negative, b >0. Therefore, the
t
learning in actor-critic methods, the policy and target
actionsshouldnotresultinanegativecashbalance,toachieve
networks are updated so slowly that they make similar
that, the environment prioritizes the execution of sell actions
estimates and offered slight improvement.
(a < 0) in the action vector A (Eq. 9) to guarantee cash
t t
2) Delayed Updates: The second component is added to
liquidity in the portfolio so buy actions (a > 0) would be
t
solve the residual error accumulation formed due to
fulfilled afterward. If the buy action still results in a negative
the learning process without a fixed target (estimates
balance(i.e.,notenoughcashtofulfilltheaction),itisfulfilled
instead). In Critic-Actor methods, this accumulation of
partially with what remains in the portfolio’s cash balance.
errors is amplified due to the interaction between the
2) Short-Selling Constraint: Short selling is prohibited in
policy (actor) and value (critic) networks, where the
the designed environment, all portfolio’s positions must be
policy gradient is maximized over the value estimate.
strictly non-negative:
Delaying the policy network update, i.e., updating it
h t ={hi t|i∈N}={h0 t,h1 t,...,hN t }∈ZN + less frequently than the value network, allows the value
network to stabilize before it can be used to update
3) Zero Slippage Assumption: When the market volatility
the policy gradient. This results in a lower variance of
is high; slippage occurs between the price at which the trade
estimates and, therefore, better policy.
wasorderedandthepriceatwhichit’scompleted[26].Inthis
3) Target Policy Smoothing Regularization: The final com-
study,themarketliquidityisassumedhighenoughtomeetthe
ponent is applying a regularization strategy to the target
transaction at the same price when it was ordered [27]. This
policy by adding a small random noise and averaging
assumptionismostlyvalidinareal-worldtradingenvironment
over mini-batches. This is important to reduce the vari-
when trading in big stock markets.
anceofthetargetvalueswhenupdatingthecritic,which
4) Zero Market Impact: In financial markets, a market
causes by overfitting spikes in the value estimate.
participant impacts the market when it buys or sells an asset
which causes the price change. The impact provoked by the The agent in this paper performs daily trading operations
agent in this study is assumed to have no effect on the market and to aid the agent to understand its environment (the stock
when it performs its actions. This assumption is mostly true market),weaugmentedthestaterepresentationoftendifferent
eveninreal-lifetradingwhenthemarketvolumeisbigenough technical indicators and news sentiment scores.
to make the individual investment insignificant [27].
B. Technical Indicator
IV. DETAILSOFIMPLEMENTATION
We used the ten most famous indicators used by technical
A. The Trading Agent traders when trading in the stock market [23], we describe
Actor-Critic-based algorithms successfully solved the con- them briefly as follows:
tinuousactionspaceutilizingfunctionapproximationandpol- 1) Relative Strength Index (RSI) ∈ RN: A momentum
+
icygradientmethods.Oneofthemostfamousactor-critic,off- indicator to measure the magnitude of recent price
policy algorithms is the Deep Deterministic Policy Gradient changes and identify overbought or oversold conditions
algorithm (DDPG) [28]. Still, despite the excellent perfor- in the stock price.
mance DDPG achieved in continuous control problems, it has 2) Simple Moving Average (SMA) ∈ RN: An important
+
a significant drawback similar to many RL algorithms, which indicatortoidentifycurrentpricetrendsandthepotential
istheoverestimationofactionvalues(max Q(s ,a ))as for a change in an established trend.
a t+1 t+1
a result of function approximation error. This overestimation 3) Exponential Moving Average (EMA) ∈ RN: Like
+
bias is unavoidable in RL as we use estimates instead of SMA, EMA is a technical indicator used to spot cur-
groundtruthinthelearningprocess.Inthisstudy,asourprob- rent trends over time. However, EMA is considered an
lem has a continuous space of actions, we use Twin Delayed improved version of SMA by giving more weight to the
Deep Deterministic Policy Gradient (TD3) [20] algorithm, recent prices considering old price history less relevant;
which is a direct successor of DDPG but with improvements thereforeitrespondsmorequicklytopricechangesthan
to tackle the overestimation problem. TD3 can reduce the SMA.
Algorithm: Twin Delayed Deep Deterministic Policy 10) Disparity Index ∈ RN: Its value is a percentage that
+
Gradient (TD3) [20] indicatestherelativepositionofthecurrentclosingprice
of the stock to a selected moving average. In this study,
1. Initialization
the selected moving average is the EMA of the look-
Critic networks Q(s,a|w ), Q(s,a|w ) and actor
1 2
back window (W).
π(s|θ), randomly, with weights W ,W and θ.
1 2
Target networks Q(cid:48), Q(cid:48) and π(cid:48) with weights
1 2
W(cid:48) ←−W ,W(cid:48) ←−W ,θ(cid:48) ←−θ C. Sentiment Scores
1 1 2 2
Replay buffer D Thesupplyanddemandfluctuationsinthestockmarketare
2. foreach t=1 to T do highly sensitive to the moment’s news due to the impact of
Initialize a random process N for action massmediaontheinvestor’sbehavior.Hencemanytradersand
exploration investorsconsiderthenewsreportsintheirstock-pickingstrat-
Select action with exploration noise egy. In our proposed approach, we believe that incorporating
a∼π(s|θ)+(cid:15),(cid:15)∼N(0,σ) the general news sentence towards the asset being considered
Observe reward r and next state s(cid:48) in the observation (state) definition will help the agent learn a
Store transition tuple (s, a, r, s(cid:48)) in D better trading strategy. In Ding et al. [31] study, they showed
Sample mini-batch of N transitions (s, a, r, s(cid:48)) that news headlines are more useful in forecasting than using
from D the entire news article content. Therefore, we only consider
a˜←π(s(cid:48)|θ)+(cid:15),(cid:15)∼clip(N(0,σ˜),−c,c) news headlines as our input to calculate the sentiment score.
y ←r+γmin Q(s(cid:48),a˜|w ) We describe the process of calculating a sentiment score for
i=1,2 i
Update critics eachassetintheportfolioattimestept(day)asthefollowing:
W i ←argmin WiN−1(cid:80) (y−Q Wi(s,a))2 • We use a rule-based matching approach to search for
if t mode d then the asset name, stock symbol, or other keywords in the
Update θ by the deterministic policy gradient: headline news (ex. Microsoft or MSFT, tech,..) released
∇ θJ(θ)= on day t.
N−1(cid:80)
∇ aQ W1(s,a)| a=πθ(s)∇ θπ θ(s) • Then we use a fine-tuned BERT model called FinBERT
Update target networks: [32] to calculate the sentiment probability (Positive,
W i(cid:48) ←τW i+(1−τ)W i(cid:48) Negative, or Neutral) of each news headline. FinBERT
θ(cid:48) ←τθ+(1−τ)θ(cid:48) model is a pre-trained NLP model to analyze sentiments
specifically for financial text.
• Finally,wetaketheaverageoftheasset’snewssentiment
probabilities for each day and assign 1 if the positive
4) Stochastic Oscillator (%K) ∈ RN: It’s a momentum probability is higher than the negative probability and -1
+
otherwise.Weignoretheneutralprobabilityaswebelieve
indicator comparing the closing price of the stock to a
that if an asset has been mentioned on the news, it will
range of its prices in a look-back window period W.
impact the asset price (positively or negatively). If the
5) MovingAverageConvergence/Divergence(MACD)∈
RN: Is one of the most used momentum indicators to asset has no news on a given day, we assign 0 to the
sentiment score.
identify the relationship between two moving averages
of the stock price and it helps the agent to understand
V. EXPERIMENTSANDRESULTS
whether the bullish or bearish movement in the price is
strengthening or weakening [29]. We evaluate our proposed approach by performing two
6) Accumulation/Distribution Oscillator (A/D) ∈ RN: differentback-testingwhichistheprocessusedbytradersand
A volume-based cumulative momentum indicator that analysts to assess the viability of a trading strategy by testing
helps the agent to assess whether the stock is being it on historical data.
accumulated(bought)ordistributed(sold)bymeasuring We perform two different back-testing experiments, the
the divergences between the volume flow and the stock purposeofthefirstexperiment(Section.V-B)istovalidatethe
price. superiority of the continuous action space to solve the trading
7) On-Balance Volume Indicator (OBV) ∈RN: Another problem by comparing the results of the same experiments
volume-based momentum indicator that uses volume reported by Kaur [39]. In their paper a discrete action space
flow to predict the changes in stock price [30]: isadaptedtosolvetheproblem,wheretheagentcanchooseto
8) Price Rate Of Change (ROC) ∈ RN: A momentum- buy, sell or hold action (i.e., discrete action space) of a fixed
based indicator that measures the speed of stock price number of shares on each time step for a portfolio of two
changes over the look-back window W. assets, namely; Qualcomm (QCOM) and Microsoft (MSFT).
9) William’s %R ∈RN: Known also as Williams Percent Weback-testourapproachonthesame5-yearsdailyhistorical
+
Range, is a momentum indicator used to spot entry and stock data (between 2011-2016) used in their study with the
exitpointsinthemarketbycomparingtheclosingprice same amount of initial capital ($10,000).
of the stock to the high-low range of prices in the look- The second experiment (Section. V-C) is conducted to
back window (W). validate the robustness of our model on large space of actions
andstatesbyconsidering10differentassetsintheportfolio.In different factors such as the actions the agent randomly starts
addition, considering that the first experiment was on training with and uses to explore or the random weight initialization.
data set only, we evaluate the performance on an unseen As suggested in [37] to ensure fairness and reliability of
market data (test data set) to check the agent’s ability of our results, we average multiple runs over different random
generalization. seeds to have an insight into the population distribution
Weusetwometricstoevaluateourresults:thefirstmetricis of the algorithm performance on an environment. In this
the cumulative sum of reward, i.e., the total profits at the end experiment’s evaluations, we report and highlight results
of the trading episode. The second metric is the annualized across several independent runs. While the recommended
Sharperatio[38]thatcombinesthereturnandtherisktogive number of trials to evaluate an RL algorithm is still an open
the average of the risk-free return by the portfolio’s deviation. question in the field, we reported the mean and standard error
Ingeneral,aSharperatioabove1.0isconsideredtobe“good” across five trials (runs), which is the suggested number in
byinvestorsbecausethissuggeststhattheportfolioisoffering many studies [37].
excess returns relative to its volatility. A Sharpe ratio higher
than 2.0 is rated as “very good” where a ratio above 3.0 is 1) Evaluation on Baseline Environment: To evaluate the
considered “excellent”. continuousactionapproachinourmodel,wetestitbysolving
the problem with only the close price of the assets as a
A. Data Description and Preprocessing market signal; hence the state representation in this baseline
environment consists of only the position state and the close
In this work, We use Yahoo Finance [33] to retrieve
price of the asset at t (C ) as a market signal, i.e., the agent
historical market daily prices. The retrieved historical data t
will solely make its trading decisions based on merely the
consists of 7 columns; Date, Volume, Open, Close, Adjusted
closing price of the stock as a market feature. We perform
Close, High and Low prices. To prepare each dataset to be
5 experiment trials each with 200 epochs (episodes) for the
usedbythemodel,wefirstperformtimestampsprocessingby
same hyperparameter configuration, only varying the random
using the trading calendar (exchange-calendars package [34])
seed across trials.
to check if the market was open on the given dates to the
Fig. 2 shows the average return (sum of rewards) at each
agent and exclude weekends and holidays from the dataset
trading episode and the standard error across the 5 runs. As
so the agent will not face gaps in the trading process. Further
can be observed, the agent’s performance increases with more
datasetprocessingisrequiredtoensurethatallfinancialassets
experience it gains with the number of epochs to successfully
(stocks) considered in the portfolio have an equal length of
achieve 33960$ average return (profits) with standard error
historical data points. Some stocks have been recorded for
equals to ±4473$. From the commission spent by the agent,
decades,whileothernewlylistedstocksareonlyafewmonths.
we can conclude that the agent was successfully able to find
This time-dimension alignment of stocks’ historical data will
a balanced trading strategy by balancing between trading and
prevent the bias action of the agent towards the stock with
holding positions. Finally, the average annual Sharpe ratio of
more data. Once we have the timestamps processed we use
our approach on the baseline environment was 1.43 with a
Close, High, Low prices and Volume at each timestamp to
standard error of ±0.13. This is significantly higher than the
calculate the technical indicators of each asset with a look-
reported Sharpe ratio 0.85 in [39] benchmark.
back window (W).
To obtain a comprehensive and accurate financial news,
2) Evaluation on WithTechIndicators Environment: Using
we combined headline news from Benzinga, Seeking Alpha,
the same configurations used in baseline environment evalua-
Zacks and other financial news websites [35], and crawled
tion, we augment the state with technical indicators and run 5
historical news headlines from Reddit worldNews Channel
independent experiments to report the average return, Sharpe
[36]. The final dataset consists of 3,288,724 news headlines
ratio, and commission. We refer to this environment with
rangingbetween2009-2021,whichweutilizedtocalculatethe
technical indicators and close price in the state representation
sentiment score.
as WithTechIndicators environment.
The results in Fig. 3 demonstrate that augmenting the en-
B. First Experiment
vironment with technical indicators has brought more helpful
Inthefirstexperiment,weconductthreeevaluationssimilar information to the agent to make better decisions. The agent
to the benchmark paper [39]. All three evaluations share the successfully achieved 89782$ average return (profits) with
same configurations like the number of assets in the portfo- ±18980$ standard error, and an average Sharpe ratio equals
lio, initial capital, commission rates, etc. but with different 2.75 with a standard error ±0.43. We can also notice that
componentsoftheenvironment’sstaterepresentation.Westart the average amount of commission is almost two times the
with a baseline which only contains the close price as a amount spent in the baseline environment, which means that
market signal feature, we then add technical indicators in theagentwassignificantlymoreactiveinbuying/sellingstocks
the second evaluation, and finally, we evaluate by adding and closed more successful deals. In addition, our approach
sentiment analysis scores. In Table. I we summarize the three outperformed the benchmark reported Sharpe ratio of 1.4.
evaluations results of the experiment.
Due to the stochasticity in the learning process, the 3) Evaluation on WithSentiments Environment: We refer
experiment results may change at each run depending on to this environment with sentiment analysis scores, technical
Fig. 2. TD3 agent performance metrics on Baseline environment using the same hyperparameter configurations averaged over 5 different random seeds.
a)Averagereturn(Profitsindollars)attheendofeachepisode.b)TheaverageannualSharperatioattheendofeachepisode.c)Theaverageamountof
commissionspentattheendofeachepisode
Fig.3. TD3agentperformancemetricsonWithTechIndicatorsEnvironment Fig. 4. TD3 agent performance metrics on WithSentiments Environment
using the same hyperparameter configurations averaged over 5 different using the same hyperparameter configurations averaged over 5 different
randomseeds.a)Averagereturn(Profitsindollars)attheendofeachepisode. randomseeds.a)Averagereturn(Profitsindollars)attheendofeachepisode.
b)TheaverageannualSharperatioattheendofeachepisode.c)Theaverage b)TheaverageannualSharperatioattheendofeachepisode.c)Theaverage
amountofcommissionspentattheendofeachepisode amountofcommissionspentattheendofeachepisode
indicators, and close price in the state representation as With- We notice in all plots of the three evaluations that the policy
Sentiments environment. We include the sentiment scores of improves over time, as the agent accumulates more reward,
news headlines for each asset in the state representation and and thus the Sharp ratio increases. Towards the end the slope
repeat the experiment with the same configurations. The total is almost flat indicating that the policy has stabilized to local
averagereturnprofitsincreasedto115591$withstandarderror optimum. As the stock trading problem has never been solved
equals to ±17721 across the five runs. Sharpe ratio increased we do not have a specified reward or Sharpe ratio threshold
to 3.14 and ±0.40 standard error. The average amount of at which it’s considered solved.
commission equals the amount spent in the environment with
only technical indicators (WithTechIndicators environment), TABLEI
whichmeansthattheagentperformedalmostthesamenumber THEPERFORMANCEEVALUATIONCOMPARISONBETWEEN
THREEDIFFERENTEVALUATIONSANDBENCHMARK
of trades but with a better decision (policy). In the bench-
mark [39] study, they also reported an increase in the agent EvaluationEnvironment Baseline WithTechIndicators WithSentiments
performance when adding sentiment scores to the state with AccumulatedReturn 33960$±4473 89782$±18980 115591$±17721
a Sharpe ratio equal to 2.4. The plot showing the results in SharpeRatio 1.43±0.13 2.75±0.43 3.14±0.4
Fig. 4 demonstrates that augmenting the state with sentiment Commission 355$±83 1109$±248 1447$±268
SharpeRation
analysisalongwithtechnicalindicatorshasimprovedtheagent 0.85 1.4 2.4
benchmark
performance.
Experiment’s Summary
C. Second Experiment ysis to find an optimal trading policy for assets in the stock
In the second experiment, we evaluate our approach on a market. Results show that the addition of technical indicators
wideractionandstatespacesbyconsidering10assetstotrade, and sentiment scores of the news headlines to the state repre-
AAPL, MSFT, QCOM, IBM, RTX, PG, GS, NKE, DIS and sentation has significantly improved the agent’s performance
AXP. and the superiority of using a continuous action space over a
Our back-testing uses historical daily data from 01/01/2010 discreteonetosolvethetradingproblem.Wealsoexploredthe
to 01/01/2018 with initial capital of 100000$ for performance potentialofusinganActor-Criticalgorithm(TD3)tosolvethe
evaluation. We split the data set into two periods, the first portfolioallocationproblem.Ourapproachachievedanannual
period is to train the agent, the second is used to test the Sharpe ratio of 2.68 on test data, which is considered ”Good”
performance of the agent on unseen data (Fig. 5). by investors. The approach can be improved in future work
byhavingmorecomputationalpowertorunmoreexperiences
andbetterevaluatetheapproach.Ourenvironment,agent,and
learning process possess many hyperparameters that must be
tuned. It will be interesting to see the model’s performance
withbetter-tunedparameters,whichrequireshighcomputation
power. In addition, we believe that training an NLP algorithm
Fig.5. Train,andtestdatasplits
to process the financial news content instead of only the
headline may positively affect the agent performance.
We notice that for our model to generalize better, we had
toimposeregularizationbynormalizingtheobservationspace
using Batch Normalization. This technique uses mini-batches
from samples to have unit mean and variance. It maintains a REFERENCES
runningmovingaverageofthemeanandvariancetonormalize
the observation vector during testing. We further normalized [1] J. Patel, S. Shah, P. Thakkar, and K. Kotecha, “Predicting stock and
stockpriceindexmovementusingtrenddeterministicdatapreparationand
the rewards received by the agent as it makes the gradient
machinelearningtechniques,”ExpertSystemswithApplications,vol.42,
steeperforbetterrewards.Wealsosetthelook-backwindowto no.1,pp.259–268,2015.
20(W =20).Weaddedactionnoisetoencourageexploration [2] A. Tsantekidis, N. Passalis, A. Tefas, J. Kanniainen, M. Gabbouj, and
during training to force the agent to try different actions and A. Iosifidis, “Forecasting stock prices from the limit order book using
convolutionalneuralnetworks,”in2017IEEE19thConferenceonBusi-
explore its environment more effectively, leading to higher
nessInformatics(CBI),vol.01,pp.7–12,2017.
rewards and more elegant behaviors. [3] A.Ntakaris,J.Kanniainen,M.Gabbouj,andA.Iosifidis,“Mid-pricepre-
Our approach successfully archived a 2.68 Sharpe ratio dictionbasedonmachinelearningmethodswithtechnicalandquantitative
indicators,”PLOSONE,vol.15,pp.1–39,062020.
which considered “very good” and 110308$ as total profits
[4] Y. Hao and Q. Gao, “Predicting the trend of stock market index using
(Rewards) on the test data. We let the agent keep learning on thehybridneuralnetworkbasedonmultipletimescalefeaturelearning,”
the test set since this will help the agent better adapt to the AppliedSciences,vol.10,no.11,2020.
market dynamics. [5] M.M.L.dePrado,“The10reasonsmostmachinelearningfundsfail,”
WGSRN:DataCollection&EmpiricalMethods(Topic),2018.
Disabling Sell Action: To investigate whether profits made
[6] T.L.MengandM.Khushi,“Reinforcementlearninginfinancialmarkets,”
on the test data (between 2016 and 2018) are a matter of Data,vol.4,no.3,2019.
the standard increase in the stocks and the market growth [7] M.L.Puterman,Markovdecisionprocesses:discretestochasticdynamic
programming. JohnWiley&Sons,2010.
in general (primarily that tech stocks are known for their
[8] S.Chakraborty,“Capturingfinancialmarketstoapplydeepreinforcement
excellent performance in the past years) or are made due to
learning,”2019.
the decision made by the agent, we disable the sell action and [9] M.R.Vargas,C.E.M.dosAnjos,G.L.G.Bichara,andA.G.Evsukoff,
onlylettheagentbuyandholdduringthetradingepisode.As “Deepleamingforstockmarketpredictionusingtechnicalindicatorsand
financialnewsarticles,”in2018InternationalJointConferenceonNeural
a result the agent allocated the capital as follows:
Networks(IJCNN),pp.1–8,2018.
[10] M. Corazza and F. Bertoluzzo, “Q-learning-based financial trading
Stock Shares systems with applications,” Working Papers 2014:15, Department of
AAPL 751 Economics,UniversityofVenice”Ca’Foscari”,2014.
AXP 398 [11] Z.Tan,C.Quek,andP.Y.Cheng,“Stocktradingwithcycles:Afinancial
MSFT 398 application of anfis and reinforcement learning,” Expert Systems with
IBM 200 Applications,vol.38,no.5,pp.4741–4755,2011.
DIS 199 [12] Y. Deng, F. Bao, Y. Kong, Z. Ren, and Q. Dai, “Deep direct rein-
forcementlearningforfinancialsignalrepresentationandtrading,”IEEE
TransactionsonNeuralNetworksandLearningSystems,vol.28,no.3,
and hold on to this position until the end of the episode.
pp.653–664,2017.
The Sharpe ratio has decreased to 2.00 with 66949$ as total [13] O.Alagoz,H.Hsu,A.J.Schaefer,andM.S.Roberts,“Markovdecision
profits (Rewards), which indicates that the decision made by processes: A tool for sequential decision making under uncertainty,”
MedicalDecisionMaking,vol.30,no.4,p.474–483,2009.
the agent had a positive effect on the return and it was not
[14] R. S. Sutton, F. Bach, and A. G. Barto, Reinforcement Learning: An
merely due to the natural growth of the market.
Introduction. MITPressLtd,2018.
[15] R. E. Bellman, Dynamic programming. Princeton University Press,
VI. CONCLUSIONANDFUTUREWORKS 2010.
[16] V.Mnih,K.Kavukcuoglu,D.Silver,A.Graves,I.Antonoglou,D.Wier-
This work presented a Deep Reinforcement Learning ap-
stra,andM.Riedmiller,“Playingatariwithdeepreinforcementlearning,”
proachthatcombinestechnicalindicatorswithsentimentanal- 2013.
[17] V.Mnih,K.Kavukcuoglu,D.Silver,A.Rusu,J.Veness,M.Bellemare,
A. Graves, M. Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen,
C.Beattie,A.Sadik,I.Antonoglou,H.King,D.Kumaran,D.Wierstra,
S.Legg,andD.Hassabis,“Human-levelcontrolthroughdeepreinforce-
mentlearning,”Nature,vol.518,pp.529–33,022015.
[18] H. van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning
withdoubleq-learning,”CoRR,vol.abs/1509.06461,2015.
[19] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D.Silver,andD.Wierstra,“Continuouscontrolwithdeepreinforcement
learning,”2019.
[20] S.Fujimoto,H.vanHoof,andD.Meger,“Addressingfunctionapprox-
imationerrorinactor-criticmethods,”CoRR,vol.abs/1802.09477,2018.
[21] F. Bertoluzzo and M. Corazza, “Testing different reinforcement learn-
ing configurations for financial trading: Introduction and applications,”
ProcediaEconomicsandFinance,vol.3,pp.68–77,2012. International
Conference Emerging Markets Queries in Finance and Business, Petru
MaiorUniversityofTˆırgu-Mures,ROMANIA,October24th-27th,2012.
[22] L.ConegundesandA.C.M.Pereira,“Beatingthestockmarketwitha
deep reinforcement learning day trading system,” in 2020 International
JointConferenceonNeuralNetworks(IJCNN),pp.1–8,2020.
[23] C. Kirkpatrick and J. R. Dahlquist, “Technical analysis: The complete
resourceforfinancialmarkettechnicians,”2006.
[24] J.R.Nofsinger,“Theimpactofpublicinformationoninvestors,”Journal
ofBanking&Finance,vol.25,no.7,pp.1339–1366,2001.
[25] Z.Xiong,X.-Y.Liu,S.Zhong,H.Yang,andA.Walid,“Practicaldeep
reinforcementlearningapproachforstocktrading,”2018.
[26] “Investopedia – slippage definition.” https://www.investopedia.com/
terms/s/slippage.asp. [Online;accessed02-October-2021].
[27] Z.Jiang,D.Xu,andJ.Liang,“Adeepreinforcementlearningframework
forthefinancialportfoliomanagementproblem,”2017.
[28] A.Akhmetzyanov,R.Yagfarov,S.Gafurov,M.Ostanin,andA.Klim-
chik, “Continuous control in deep reinforcement learning with direct
policy derivation from q network,” in Human Interaction, Emerging
TechnologiesandFutureApplicationsII,(Cham),pp.168–174,Springer
InternationalPublishing,2020.
[29] T. T.-L. Chong, W.-K. Ng, and V. K.-S. Liew, “Revisiting the per-
formance of macd and rsi oscillators,” Journal of Risk and Financial
Management,vol.7,no.1,pp.1–12,2014.
[30] J. Granville, Granville’s New Key to Stock Market Profits. Papamoa
Press,2018.
[31] X. Ding, Y. Zhang, T. Liu, and J. Duan, “Using structured events to
predictstockpricemovement:Anempiricalinvestigation,”inProceedings
of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP), (Doha, Qatar), pp. 1415–1425, Association for
ComputationalLinguistics,Oct.2014.
[32] D. Araci, “Finbert: Financial sentiment analysis with pre-trained lan-
guagemodels,”2019.
[33] “Yahoofinance.”https://finance.yahoo.com/.
[34] G. Manoim, “exchange-calendars.” https://pypi.org/project/
exchange-calendars/.
[35] “Kaggle–dailyfinancialnewsfor6000+stocks.”https://www.kaggle.
com/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests.
[Online;accessed15-November-2021].
[36] “Kaggle – sun, j. (2016, august). daily news for stock market predic-
tion.” https://www.kaggle.com/aaron7sun/stocknews. [Online; accessed
15-November-2021].
[37] P.Henderson,R.Islam,P.Bachman,J.Pineau,D.Precup,andD.Meger,
“Deepreinforcementlearningthatmatters,”2019.
[38] W.F.Sharpe,“Thesharperatio,”TheJournalofPortfolioManagement,
vol.21,no.1,pp.49–58,1994.
[39] S. Kau, “Algorithmic trading using reinforcement learning augmented
withhiddenmarkovmodel.workingpaper,stanforduniversity.,”2017.
