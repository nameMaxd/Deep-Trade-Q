# Полный аудит торгового бота

## 1. Цели и общий обзор

Этот документ содержит глубокий анализ текущей реализации торгового бота на основе TD3, его поведенческих результатов, сильных и слабых сторон, а также набор конкретных рекомендаций по улучшению стабильности и прибыльности.

## 2. Архитектура среды (`TradingEnv`)

- **Фичи:**
  - Скользящие окна цен: нормированные разности цен и индикаторы (SMA, EMA, RSI).
  - Объёмы: относительный объём к среднему.
  - Моментум, волатильность.
- **Ограничения:**
  - Комиссия 0.1% за сделку.
  - Стоимость удержания позиции (carry cost) = 0.0001.
  - Штраф за холд (hold penalty) = 0.1.
  - Максимум позиций = 8 лотов.
  - Минимальная сумма на сделку = $2.5 (дробное кол-во акций).
- **Преимущества:** натурализм торговых издержек; управление рисками через штрафы.
- **Недостатки:** отсутствует учёт проскальзывания, спреда, временных лагов.

## 3. Архитектура агента и процесс обучения

- **Алгоритм:** TD3 (off-policy, два критика, policy delay).
- **Политика:** MlpPolicy (двухслойная сеть, скрытые размеры по умолчанию).
- **Гиперпараметры:**
  - lr = 3e-4, batch_size = 256, буфер = 1e6.
  - Шум Нормальный σ = 1.0.
- **Механизмы контроля:** ранний стоп (patience=3), частота оценки = каждые 5000 шагов, n_eval=3.
- **Финальная проверка:** детерминированное исследование (deterministic=True), вывод итоговых профитов.

## 4. Анализ результатов (Train vs Validation)

| Шаг    | TrainProfit | ValProfit | Комментарий                               |
|--------|-------------|-----------|--------------------------------------------|
|  5 000 |  348.95     |  42.63    | минимальный рост; признаки застревания.     |
| 10 000 |  641.56     |  43.77    | рост переобучения, валидация на месте.     |
| 15 000 | 2 972.32    |  46.82    | сильный скачок на трейне, но валидация не растёт. |
| 20 000 |  559.93     |  38.41    | резкий откат; режим переобучения.         |

**Вывод:** бот демонстрирует сильный overfitting: TrainProfit растёт нестабильно, ValProfit остаётся низким (~40–50$) при реальных издержках.

## 5. Критика функции награды

- **Плюсы:** учёт комиссий, стоимости удержания; вербальное поощрение при росте позиции.
- **Минусы:**
  - Нет учёта волатильности и рисковых показателей (Sharpe, drawdown).
  - Награда выдаётся локально (в момент продажи/держания), а не по кумулятивному риску.
  - Пользователь может накапливать частые мелкие выигрыши, игнорируя крупные потери.

**Рекомендации:**
1. Добавить штраф за высокий волатильный риск (критерий Sharpe).
2. Ввод max drawdown penalty.
3. Разделить reward на эксплуатацию и exploration-фазы (двухфазная стратегия).

## 6. Критика сетевой архитектуры

- **Ограничения MLP:**
  - Нет явной памяти о долгосрочных паттернах, только фиксированное окно.
  - Слабее работает на смене рыночных режимов.
- **RNN/LSTM:** позволяют аккумулировать скрытое состояние, учитывать последовательность.
- **Transformer:** мощнее в распознавании сложных зависимостей, но ресурсоёмок.
- **Практичность:** для SB3 можно использовать `SB3-contrib` recurrent policies или кастомные сети.
  - **Минусы:** усложнение обучения, рост времени тренировки.

## 7. 30 вариантов улучшений (★ – потенциал)

1. **Sharpe-adjusted reward** — ★★★★☆
2. **Max drawdown penalty** — ★★★☆☆
3. **Симуляция проскальзывания** (spread & slippage) — ★★★☆☆
4. **Адаптивный размер позиции** (volatility-based) — ★★★★☆
5. **Доп. индикаторы** (MACD, BollingerBands) — ★★★☆☆
6. **LSTM-слой в политике** — ★★★★☆
7. **Transformer-энкодер** для history — ★★★☆☆
8. **Переключиться на SAC** (самоэнгенный) — ★★★★☆
9. **Оптуна гиперпараметры** — ★★★★★
10. **Prioritized Replay** — ★★★☆☆
11. **Анализ переобучения** (dropout, регуляризация) — ★★★☆☆
12. **Шедулинг lr** — ★★★☆☆
13. **Энсемблинг агентов** — ★★☆☆☆
14. **Walk-forward CV** — ★★★★☆
15. **Увеличить n_eval_episodes** — ★★☆☆☆
16. **Имитация новостного фона** — ★☆☆☆☆
17. **Риск-менеджмент (max daily loss)** — ★★★★☆
18. **On-policy алгоритмы (PPO)** — ★★★☆☆
19. **Curriculum Learning** (постепенное усложнение) — ★★☆☆☆
20. **Метрики stability (MDD, Ulcer Index)** — ★★★☆☆
21. **Meta-learning** для смены режимов — ★☆☆☆☆
22. **Feature engineering** (нормировка z-score) — ★★★☆☆
23. **Ввод альтернативных данных** — ★★☆☆☆
24. **Тонкая настройка window_size** — ★★☆☆☆
25. **Динамическое окно** — ★★☆☆☆
26. **RLHF/имитационное обучение** — ★☆☆☆☆
27. **Risk-aware policy** (констрейнты) — ★★★★☆
28. **Temporal ensemble** — ★★☆☆☆
29. **Regularized networks** (dropout, weight decay) — ★★☆☆☆
30. **Мониторинг и визуализация** (dashboard) — ★★★★★
