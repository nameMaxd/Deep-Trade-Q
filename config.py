"""
Конфигурационный файл для торгового агента Deep-Trade-Q
Содержит настройки для TD3 модели и торговой среды
"""

import numpy as np

# Конфигурация окружения (TradingEnv)
ENV_CONFIG = {
    # Размер окна для наблюдения (количество дней для анализа)
    "window_size": 50,
    
    # Комиссия за сделку (доля от суммы сделки)
    # Реальная комиссия на акциях Google (Interactive Brokers)
    "commission": 0.0035,  # 0.35% - реалистичная комиссия с учетом спреда и проскальзывания
    
    # Максимальное количество позиций, которые можно держать одновременно
    "max_inventory": 2,  # Уменьшено с 4 до 2
    
    # Стоимость удержания позиции (штраф за каждый шаг удержания)
    # Увеличено, чтобы наказывать за долгое удержание
    "carry_cost": 0.002,  # Увеличено в 4 раза
    
    # Минимальная стоимость сделки в долларах
    "min_trade_value": 10.0,
    
    # Параметры риска
    "risk_lambda": 0.3,  # Вес риска в функции награды
    "drawdown_lambda": 0.3,  # Вес просадки в функции награды
    
    # Режим обучения (чередование фаз исследования и эксплуатации)
    "dual_phase": True,
    
    # Максимальная длина эпизода (шаги)
    # Используем весь датасет для каждого эпизода
    "max_episode_steps": None,  # None = весь датасет
    
    # Штраф за удержание позиции (для поощрения активной торговли)
    "hold_penalty": 0.1,  # Увеличено в 5 раз
    
    # Бонус за закрытие позиции с прибылью
    "profit_bonus": 1.0,  # Увеличено в 2 раза
    
    # Штраф за закрытие позиции с убытком
    "loss_penalty": 0.1,  # Оставляем как есть
    
    # Параметр для наказания за стратегию "купи и держи"
    "long_hold_penalty_factor": 0.01,  # Увеличено в 10 раз
    
    # Параметр для поощрения частых сделок
    "trade_frequency_bonus": 0.2,  # Увеличено в 2 раза
    
    # Параметр для наказания за большие просадки
    "max_drawdown_penalty": 0.5,  # Штраф за максимальную просадку
    
    # Нормализация цен для устранения разрыва между историческими и текущими ценами
    "normalize_prices": False  # Отключаем, т.к. теперь нормализуем вручную
}

# Конфигурация TD3 модели
TD3_CONFIG = {
    # Общие параметры обучения
    "timesteps": 10000,  # Общее количество шагов обучения
    "learning_rate": 3e-4,  # Скорость обучения (3e-4 = 0.0003)
    "batch_size": 256,  # Размер батча для обучения
    "buffer_size": 100000,  # Размер буфера опыта
    
    # Параметры шума для исследования
    # Чем выше sigma, тем больше исследования (более случайные действия)
    # 0.1 - холодный (мало исследования)
    # 0.3 - теплый (среднее исследование)
    # 0.5 - горячий (много исследования)
    "noise_sigma": 0.3,
    
    # Архитектура нейронной сети
    # pi - сеть политики (actor)
    # qf - сеть ценности (critic)
    "net_arch": {
        "pi": [256, 128, 64],  # Три слоя с уменьшающимся количеством нейронов для политики
        "qf": [256, 128, 64]   # Три слоя с уменьшающимся количеством нейронов для ценности
    },
    
    # Активационная функция
    # Варианты: "relu", "tanh", "elu"
    "activation_fn": "relu",
    
    # Частота обучения
    # (1, "episode") - обучение после каждого эпизода
    # (1, "step") - обучение после каждого шага
    "train_freq": (1, "episode"),
    
    # Параметры оценки
    "eval_freq": 1000,  # Частота оценки (каждые N шагов)
    "n_eval_episodes": 2  # Количество эпизодов для оценки
}

# Конфигурация визуализации
VIS_CONFIG = {
    "max_plots": 10,  # Максимальное количество графиков
    "save_path": "plots"  # Путь для сохранения графиков
}
