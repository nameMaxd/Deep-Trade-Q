"""
Конфигурационный файл для торгового агента Deep-Trade-Q
Содержит настройки для TD3 модели и торговой среды
"""

import numpy as np

# Конфигурация окружения (TradingEnv)
ENV_CONFIG = {
    # Размер окна для наблюдения (количество дней для анализа)
    "window_size": 50,
    
    # Комиссия за сделку (доля от суммы сделки)
    # 0.001 = 0.1% комиссия
    "commission": 0.001,
    
    # Максимальное количество позиций, которые можно держать одновременно
    # Ограничивает риск и заставляет модель быть более избирательной
    "max_inventory": 8,
    
    # Стоимость удержания позиции (штраф за каждый шаг удержания)
    # Увеличено в 10 раз, чтобы наказывать за долгое удержание
    "carry_cost": 0.001,
    
    # Минимальная стоимость сделки в долларах
    # Предотвращает открытие слишком маленьких позиций
    "min_trade_value": 10.0,
    
    # Параметры риск-аверсии
    # Чем выше значение, тем больше модель избегает риска
    "risk_lambda": 0.1,  # Вес риска в функции награды
    "drawdown_lambda": 0.1,  # Вес просадки в функции награды
    
    # Режим обучения
    # True - чередование фаз исследования и эксплуатации
    # False - только эксплуатация
    "dual_phase": True,
    
    # Максимальная длина эпизода (шаги)
    # Уменьшаем, чтобы агент не мог держать позиции слишком долго
    "max_episode_steps": 500,
    
    # Штраф за удержание позиции (для поощрения активной торговли)
    "hold_penalty": 0.05,
    
    # Бонус за закрытие позиции с прибылью
    "profit_bonus": 0.2,
    
    # Штраф за закрытие позиции с убытком (меньше, чем бонус за прибыль)
    "loss_penalty": 0.1
}

# Конфигурация TD3 модели
TD3_CONFIG = {
    # Общие параметры обучения
    "timesteps": 10000,  # Общее количество шагов обучения
    "learning_rate": 3e-4,  # Скорость обучения (3e-4 = 0.0003)
    "batch_size": 256,  # Размер батча для обучения
    "buffer_size": 100000,  # Размер буфера опыта
    
    # Параметры шума для исследования
    # Чем выше sigma, тем больше исследования (более случайные действия)
    # 0.1 - холодный (мало исследования)
    # 0.3 - теплый (среднее исследование)
    # 0.5 - горячий (много исследования)
    "noise_sigma": 0.3,
    
    # Архитектура нейронной сети
    # pi - сеть политики (actor)
    # qf - сеть ценности (critic)
    "net_arch": {
        "pi": [256, 128, 64],  # Три слоя с уменьшающимся количеством нейронов для политики
        "qf": [256, 128, 64]   # Три слоя с уменьшающимся количеством нейронов для ценности
    },
    
    # Активационная функция
    # Варианты: "relu", "tanh", "elu"
    "activation_fn": "relu",
    
    # Частота обучения
    # (1, "episode") - обучение после каждого эпизода
    # (1, "step") - обучение после каждого шага
    "train_freq": (1, "episode"),
    
    # Параметры оценки
    "eval_freq": 1000,  # Частота оценки (каждые N шагов)
    "n_eval_episodes": 2  # Количество эпизодов для оценки
}

# Конфигурация визуализации
VIS_CONFIG = {
    "max_plots": 10,  # Максимальное количество графиков
    "save_path": "plots"  # Путь для сохранения графиков
}
